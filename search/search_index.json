{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":""},{"location":"#intro","title":"Intro","text":"<p>Hi, my name is Oscar Ng \ud83d\udc4b</p> <p>This is my personal wiki, a little digital garden where my experiences, knowledges, opinions and thoughts are cumulated. They may be outdated (or biased), but I'll try my best to keep them up to date! </p> Open to see how dots interconnects themselves! \ud83d\ude80"},{"location":"#credit","title":"Credit","text":"<p>Shout out to Jobin Join for the useful template!</p>"},{"location":"2024-01-16/","title":"2024-01-16","text":""},{"location":"2024-08-10/","title":"2024-08-10","text":""},{"location":"devops/git/Git%20Cheatsheet/","title":"Git Cheatsheet","text":""},{"location":"devops/git/Git%20Cheatsheet/#basic-concept","title":"Basic Concept","text":"<p>Interesting characteristics about git as follows: * Commit and Trees are in form of directed acyclic graph (DAGs), meaning no cyclic dependency  * Each Git Repo is a self-contained. Your local repo is technically separate from the remote repo * Everything is stored inside <code>.git</code> folder</p> <p></p>"},{"location":"devops/git/Git%20Cheatsheet/#commands","title":"Commands:","text":""},{"location":"devops/git/Git%20Cheatsheet/#git-add","title":"git add","text":"<p>use git add to add changes from working directory to staging area</p> <pre><code>git add * # recursively add all\ngit add individual.file\n</code></pre>"},{"location":"devops/git/Git%20Cheatsheet/#git-commit","title":"git commit","text":"<p>use git commit to commit changes from staging area to local repository</p> <pre><code>git commit -m \"my lovely change\"\n</code></pre>"},{"location":"devops/git/Git%20Cheatsheet/#git-push","title":"git push","text":"<p>use git push to push changes from local repository to upstream remote repository</p> <pre><code>git push\ngit push --set-upstream origin hello  # map local hello branch to remote hello branch\ngit push -u origin local_branch:remote_branch # map local_branch to remote branch\ngit push --force\ngit push --force-with-lease # safer option, ensure not overwritting work\n</code></pre>"},{"location":"devops/git/Git%20Cheatsheet/#git-rev-parse","title":"git rev-parse","text":"<p>Obtain git revision, such as current commit</p> <pre><code>git rev-parse @    # current commit hash\ngit rev-parse HEAD # current commit hash, too\ngit rev-parse --short HEAD # current commit hash but shorter. CANT be used for fetch\ngit rev-parse @~1  # last commit hash\ngit rev-parse @~2  # last commit hash\n</code></pre>"},{"location":"devops/jenkins/","title":"jenkins","text":""},{"location":"devops/jenkins/#jenkins","title":"jenkins","text":""},{"location":"devops/jenkins/#sections-of-jenkins","title":"Sections of jenkins","text":"<ul> <li>Jenkins Tips and Tricks</li> <li>Jenkins and Modern Practices</li> </ul>"},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/","title":"Jenkins Tips and Tricks","text":""},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/#intro","title":"Intro","text":"<p>A place for documentating all dirty little tricks for writing jenkinsfile</p>"},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/#parameters-caching","title":"Parameters caching","text":"<p>You can cache your input parameters of the previous run into your next run with the following:</p> <pre><code>// nightly teardown pipeline for bucket\npipeline {\n  parameters {\n     string( name: 'skipDeleteAWSBucket', \n     defautltValue: params.skipDeleteAWSBucket ?: '',                             \n     description: 'comma separated list of buckets to not get cloud-nuked overnight')\n  }\n}\n</code></pre> <p>This is not a good practice with the introduction of dependency based on previous run, plus anyone can easily change the value through UI input in clickOps fashion instead of GitOps.  </p> <p>However, for low-risk, internal automation with everyone acting in good faith, it can be quite handy.  With the example above,  anyone can self-service themselves simply by adding their environment name should they wish to keep their test bucket a bit longer.</p>"},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/#optional-teardown-stage","title":"Optional Teardown stage","text":"<p>I am a fan of defining my pipeline logic as 3 stages:  setup-deploy-teardown. While having teardown in <code>post { always { } }</code> by default,  making it optional is a bit tricky since it doesn't support the declarative <code>when</code> syntax within <code>post</code>. Solution:</p> <pre><code>import org.jenkinsci.plugins.pipeline.modeldefinition.Utils\n\npost {\n  always {\n    script {\n       stage('Teardown'){\n         if (!params.skipTeardown){\n           echo \"tearing things down\"\n         } else { Utils.markStageSkippedForConditional('Teardown') }\n       }\n    }\n  }\n}\n</code></pre>"},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/#load-local-groovy-file","title":"Load Local Groovy file","text":"<p>I hope your groovy logic is not too complicated \ud83d\ude07, yuck</p> <pre><code>def exampleModule = load \"${WORKSPACE}/Example.Groovy \"\n    exampleModule.exampleMethod()\n    exampleModule.otherExampleMethod()\n</code></pre> <p>Also, consider using shared libraries, see Jenkins and Modern Practices#Shared Libraries</p>"},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/#global-variable","title":"Global Variable","text":"<p>You can declare global variable outside pipeline to be used, even in declarative pipeline</p> <pre><code>def ar = (some_condition) ? \"prod-artifact-registry\" : \"dev-artifact-registry\" \n\npipeline {\n  parameters {\n     string( name: 'artifact-registry', \n     defautltValue: ar,                                   \n     description: 'artifact registry to read from')\n  }\n}\n</code></pre> <p>Alternatively, use <code>env</code>  to provide value from one stage to the subsequent one</p> <pre><code>stage('test1'){\n  script {\n    env.TEST = \"1\"\n  }\n}\nstage('test2'){\n  script {\n    echo \"$TEST\"  //1\n    env.TEST = \"2\"\n  }\n}\nstage('test'){\n  script {\n    echo \"$TEST\"  //2\n  }\n}\n</code></pre> <p>Can be hard to trace the variable value though  \ud83d\ude07</p>"},{"location":"devops/jenkins/Jenkins%20Tips%20and%20Tricks/#parallelism","title":"Parallelism","text":"<p>you can achieve parallelism with the <code>parallel</code> closure:</p> <pre><code>stage('Doing Things In Parallel'){\n  parallel {\n    stage('task1'){}\n    stage('task2'){}\n  }\n}\n</code></pre> <p>nested parallel is not supported in declarative pipeline, but you can do that with the <code>parallel</code> function that accepts a <code>Map &lt;String, Closure&gt;</code> . Do note that it messed up the Blue Ocean view:</p> <pre><code>stage('Doing Things In Parallel'){\n  parallel {\n    stage('task1'){\n      script {\n        def stages = [:]\n        stages[\"subtask 1\"] = { dir(\"/here\"){ echo \"subtask1\" } }\n        stages[\"subtask 2\"] = { sh \"subtask2\" }\n        parallel stages\n      }\n    }\n    stage('task2'){}\n  }\n}\n</code></pre> <p>and obviously, you can do parallelism within bash as well:</p> <pre><code>sleep 5 &amp;\nsleep 10 &amp;\nwait\n</code></pre> <p>or more complex and complete logic with <code>{}</code> :</p> <pre><code>#!/bin/bash\n{\n    echo \"task1\"\n    sleep 5\n    echo $?\n} &amp;&gt; task1.log\n\n{\n    echo \"task1\"\n    sleep 10\n    echo $?\n} &amp;&gt; task2.log\n\nwait\ncat task1.log\ncat task2.log\n</code></pre>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/","title":"Jenkins and Modern Practices","text":""},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#intro","title":"Intro","text":"<p>Like it or not, Jenkins is still THE most popular CI/CD framework out there, at least in 2023.  You are using Jenkins, probably because of the following reasons:</p> <ul> <li>Company is already using Jenkins</li> <li>Company wants to stick with Jenkins because hiring is easier</li> <li>Company wants to stick with Jenkins because it is free</li> <li>Company wants to stick with Jenkins because others are not as flexible</li> <li>you choose Jenkins still, fully knowing how to maintain one</li> </ul> <p>Regardless, there are many pitfalls, including but not limited to:</p> <ol> <li>Upgrading Jenkins version</li> <li>Maintaining Jenkins Plugin</li> <li>People relying too much on configuration through UIs  (ClickOps)</li> <li>People writing complex, non-declaractive scripted pipelines</li> </ol> <p>Ultimately, the biggest problem is that Jenkins can easily turn into an urban sprawl - as number of pipelines, plugins, configuration rapidly grows and interlink/depends on each other, it makes jenkins a nightmare for engineers to maintain, and nobody is happy living in this unregulated mess either. </p> <p>One way to achieve consistency for Jenkins is to treat it as a proper software-product:  make everything, from jenkinsfile, job configuration to Jenkins itself, presentable as code and go through standard validation and testing process like any software. The following sections provide some examples to achieve it</p>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#everything-as-code-the-organized-practice","title":"Everything as Code - The Organized Practice","text":""},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#deployment-as-code","title":"Deployment as Code","text":"<p>While it's possible to install Jenkins directly on bare-metal machine,  it should still be hosted like any production-workload with enough care, such as:</p> <ul> <li>deploy Jenkins as container (i.e. Docker) if possible</li> <li>install Jenkins with Helm Charts or Jenkins Operator if in Kubernetes</li> <li>Use an ansible playbook or others if deployed in Virtual machine</li> <li>Use a Virtual machine image if deployed in Virtual Machine</li> <li>have deployment script, Dockerfile and manifest available somewhere in Git-repository. </li> </ul> <p>It's a chicken-and-egg problem that CI/CD itself requires deployment, but it can optionally be deployed with other toolings instead (Terraform, Ansible, Other CI etc)</p>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#jscac","title":"Jscac","text":"<p>Jscac stands for Jenkins Configuration As Code, which allows jenkins configurations, including plugins, nodes and credentials, to be fully configurable in YAML.  This allows jenkins environment to be reproducible such that it is consistent across dev/stage/prod environment.</p>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#job-dsl","title":"Job DSL","text":"<p>Job DSL is a jenkins plugin that allows jobs (i.e. folder, pipelines) to be setup through code/automation rather than doing it through UI.  </p> <p>It's usually less important as the pipeline code itself already allows customization of the job configuration, but when there are MANY pipelines (&gt;10)  to be used regularly in production, you should consider a plugin like this</p>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#jenkinsfile","title":"Jenkinsfile","text":"<p>Jenkinsfile is a native feature in Jenkins that allows pipeline as code, similar to workflow yaml in GitHub Action.  Not much to say here, other than the recommended practice of using  declarative syntax</p> <pre><code>pipeline {\n    agent any\n    options {\n        // Timeout counter starts AFTER agent is allocated\n        timeout(time: 1, unit: 'SECONDS')\n    }\n    stages {\n        stage('Example') {\n            steps {\n                echo 'Hello World'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#shared-libraries","title":"Shared Libraries","text":"<p>Jenkins support sharing of common functions through Jenkins shared libraries. </p> <p>Personally, I am not a huge fan of it, as it requires functions to be written in groovy (which ends up being wrapper function of a bash script), and even worse: documentation written in txt file next to the groovy function that only appears after first pipeline-sccessful run.  </p> <pre><code>(root)\n+- src                     # Groovy source files\n|   +- org\n|       +- foo\n|           +- Bar.groovy  # for org.foo.Bar class\n+- vars\n|   +- foo.groovy          # for global 'foo' variable\n|   +- foo.txt             # help for 'foo' variable\n+- resources               # resource files (external libraries only)\n|   +- org\n|       +- foo\n|           +- bar.json    # static helper data for org.foo.Bar\n</code></pre> <p>However, it still brings enough benefit to the table as it standardizes some of the common function/procedure and make pipeline DRY.</p> <p>The only suggestions are:</p> <ul> <li>avoid making the logic too complex. Isolated, standalone function per <code>.groovy</code>  is enough</li> <li>make the function easy to use for cross-team consumption. e.g. <code>connectToCluster(name)</code> . Groovy does support default parameters so try to keep compulsory parameters as few as possible.</li> <li>write JUnit Test</li> </ul>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#offloading-logic-outside-pipeline","title":"Offloading logic outside pipeline","text":"<p>Not a jenkins-specific practice, but rather good CI/CD practice in general. Put the bulk of pipeline logics (e.g.  image building,  automation) outside of pipeline code, such that logics can be developed, tested  verified locally and make them transferrable across other CI/CD.  Solution includes:</p> <ul> <li>locally-runnable bash script</li> <li>Makefile, to a certain extent</li> <li>Build System like Gradle with gradle task,  npm with <code>npm scripts</code></li> <li>Language specific setups like <code>console_scripts</code> in Python</li> <li>dagger.io </li> </ul> <p>Then the abstracted logic can simply be called from CI/CD through passed command flags/parameters/environment variable.</p>"},{"location":"devops/jenkins/Jenkins%20and%20Modern%20Practices/#final-thoughts-against-the-trend","title":"Final Thoughts - Against the trend?","text":"<p>Certain practices above are easy to do, while others are not. Often times, A dedicated 'Devops' team will be needed to enforcing standard as well as maintaining the usage of Jenkins, potentailly with a (few) centralized jenkins-related repo(s), while strictly following the Jenkins-as-code philosophy above.</p> <p>This raises a lot of challenges:</p> <ul> <li>not supposed to have a dedicated Devops team!  (Devops is mindset yada yada)</li> <li>maintaining every pipeline by a central team gets intensive &amp; demanding real quick</li> <li>Challenges of versioning jenkinsfile/cicd in a centralized jenkins/cicd repo </li> <li>not everyone care about it realistically speaking, especially developers that don't want to touch CI/CD</li> <li>not agile enough if required to comply with so many standards</li> <li>centralized setup may not work well with other practices/plugin (e.g. multibranch pipeline plugin)</li> <li>centralized setup that was setup previously needs to be reevaluated as business grows</li> </ul> <p>And if you think about it, modern CI/CD like GitHub Action or Gitlab CI/CD:</p> <ul> <li>has pipeline code heavily tied to a repo as opposed to centralized pipeline repository</li> <li>expect the code owner of the repo to take ownership of their CI/CD as well</li> <li>decentralized -  Each pipeline is usually isolated/runnable by its own</li> </ul> <p>There is no perfect solution for this.  An extreme case can be dev &amp; ops side heavily siloed from each other with entirely different tools due to complete freedom, or in the opposite end of everything being too tightly controlled that hammers productivity.</p> <p>A hybrid approach can also work, but it depends on how 'hybrid' it would be:</p> <ul> <li>Tailored towards centralization - tighten security and limit user permission to specific folders</li> <li>Balance - Everything-as-code applicable only to busines-critical pipelines, others to a varying degree</li> <li>Tailored towards decentralization - Total Freedom except a set of standardized rules to follow, e.g: <ul> <li>endproduct must be a container to be uploaded in X registry</li> <li>endproduct, as part of microservice, must fulfill OpenAPI standard</li> </ul> </li> </ul> <p>Ultimately, there are a lot of decisions to be made with no right-or-wrong answer. The only way moving forward is to satisfy the requirements of majority of stakeholders as opposed to enforcing standards on them.  After all, CI/CD is for streamlining and automating development &amp; deployment process, not slowing things down. </p>"},{"location":"devops/networking/","title":"networking","text":""},{"location":"devops/networking/#networking","title":"networking","text":""},{"location":"devops/networking/#sections-of-networking","title":"Sections of networking","text":"<ul> <li>Certificates</li> <li>OSI 7 Layers</li> </ul>"},{"location":"devops/networking/Certificates/","title":"Certificates","text":""},{"location":"devops/networking/Certificates/#intro","title":"Intro","text":"<p>This page is an introduction of how certificates setups work</p>"},{"location":"devops/networking/Certificates/#basic-concept","title":"Basic Concept","text":"<p>Crypotgraphy:</p> <ul> <li>Asymetric Keys: The cryptographic scheme that supports two different keys</li> <li>Public Key: the certificate itself</li> <li>Private Key: the key that is associated with the certiciate</li> <li>encrypt:  Alice uses Bob's public key to write a msg, Bob uses Bob's private key to decrypt the msg</li> <li>sign:  Alice uses Alice's private key to write the msg, Bob uses Alice's public key to cal. and verify hash</li> </ul>"},{"location":"devops/networking/Certificates/#self-signed-certificate","title":"Self-signed certificate","text":"<p>A self signed certificate is simply a certificate that is not issued by a certificate authority but itself. This can be created either via Terraform</p> <pre><code>resource \"tls_private_key\" \"server_pem\" { \n  algorithm = \"RSA\" \n  rsa_bits = 2048 \n} \n\nresource \"tls_self_signed_cert\" \"server_crt\" { \n  private_key_pem = tls_private_key.server_pem.private_key_pem \n  dns_names = [\"example.com\"]\n  is_ca_certificate = true\n\n  subject { \n    common_name = \"example.com\" \n    organization = \"Example, Inc.\" \n  }\n\n  validity_period_hours = 24 \n}\n</code></pre> <p>or use openssl command:</p> <pre><code>openssl req -x509 -newkey rsa:2048 \\\n-keyout key.pem \\ \n-out cert.pem \\ \n-sha256 \\\n-days 3650 \\\n-nodes \\\n-subj \"/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname\"\n</code></pre>"},{"location":"devops/networking/Certificates/#locally-signed-certificate-with-private-ca","title":"Locally Signed Certificate with Private CA","text":"<p>Unlike a single self-signed certificate, you can setup your own private root CA (which is a self-signed cert by its own), and use that to sign arbitrary server certs as you need.  </p> <p>This way, the client only need to trust one root CA to verify the rest of server certs. Certain technology also only allows private certificate issued by a private CA (e.g. Azure Application Gateway)</p> <p>Use Terraform:</p> <pre><code>\nresource \"tls_private_key\" \"root_pem\" {\n algorithm = \"RSA\" \n  rsa_bits = 2048 \n}\n\nresource \"tls_self_signed_cert\" \"root_crt\" { \n  private_key_pem = tls_private_key.root_pem.private_key_pem \n  is_ca_certificate = true\n  dns_names = [\"example.com\"]\n\n  subject { \n    common_name = \"example.com root CA\" \n    organization = \"Example, Inc.\" \n  }\n\n  validity_period_hours = 24 \n}\n\n\nresource \"tls_private_key\" \"server_pem\" {\n algorithm = \"RSA\" \n  rsa_bits = 2048 \n}\n\nresource \"tls_cert_request\" \"server_cer\" {\n  private_key_pem = tls_private_key.server_pem.private_key_pem\n  dns_names = [\"mysubdomain.example.com\"]\n  subject { \n    common_name = \"mysubdomain example\" \n    organization = \"Example, Inc.\" \n  }\n}\n\nresource \"tls_locally_signed_cert\" \"server_crt\" {\n  cert_request_pem   = tls_cert_request.server_cer.cert_request_pem\n  ca_private_key_pem = tls_private_key.root_pem.private_key_pem \n  ca_cert_pem        = tls_self_signed_cert.root_crt.cert_pem\n\n  subject { \n    common_name = \"mysubdomain example\" \n    organization = \"Example, Inc.\" \n  }\n\n  validity_period_hours = 12\n}\n</code></pre> <p>or use openssl commands:</p> <pre><code>openssl genpkey -algorithm RSA -out root.key -aes256\n</code></pre> <pre><code>openssl req -x509 -new -nodes -key root.key -sha256 -days 3650 -out root.crt\n</code></pre> <pre><code>openssl genpkey -algorithm RSA -out server.key -aes256\n</code></pre> <pre><code>openssl req -new -key server.key -out server.csr\n</code></pre> <pre><code>openssl x509 -req \\\n-in server.csr \\\n-CA root.crt \\\n-CAkey root.key \\\n-CAcreateserial -out server.crt -days 365 -sha256 -extfile v3.ext\n</code></pre>"},{"location":"devops/networking/Certificates/#intermediate-cert-and-cert-chain","title":"Intermediate Cert and Cert Chain","text":"<p>Instead of Root certificate, a server cert can be signed by an intermediate cert instead, and the intermediate cert is signed by either the root cert or an upper intermediate cert. </p> <p>Ulltimately, this keeps the Root cert safe as revoking a Root cert is a lot more work than revoking an intermediate cert:</p> <ul> <li>Need to update machine clients (browser, OS) to trust such certificate</li> <li>All child certificates become not trustable</li> </ul> <p>Server needs to present:</p> <pre><code>-----BEGIN CERTIFICATE-----  \ncontent of your server certificate  \n-----END CERTIFICATE-----  \n-----BEGIN CERTIFICATE-----  \ncontent of any intermediate CA certificate  \n-----END CERTIFICATE-----\n</code></pre> <p>In some case, such as Strimzi Kafka, it would require you to present the whole chain instead:</p> <pre><code>-----BEGIN CERTIFICATE-----  \ncontent of your server certificate  \n-----END CERTIFICATE-----  \n-----BEGIN CERTIFICATE-----  \ncontent of any intermediate CA certificate  \n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----  \ncontent of the root CA \n-----END CERTIFICATE-----\n</code></pre> <p>For the client itself, all you need is to add the cert to truststore, e.g. :</p> <pre><code>keytool -import -keystore ./cacerts -trustcacerts -file cacert.pem -storepass changeit\n</code></pre>"},{"location":"devops/networking/Certificates/#https-termination","title":"HTTPS termination","text":"<p>For L7 loadbalancer that supports:</p> <ul> <li>TLS termination  (traffic is decrpyted and send over to Server), or</li> <li>end-to-end TLS encryption (traffic is decrypted and re-encrypted with new connection)</li> </ul> <p>such as Azure Application Gateway, </p> <p>you will need to provide a trusted root certificate to the backend, whilst the server will hold the intermediate &amp; server cert, as intended</p>"},{"location":"devops/networking/OSI%207%20Layers/","title":"OSI 7 Layers","text":""},{"location":"devops/networking/OSI%207%20Layers/#intro","title":"Intro","text":"<p>This page provides basic overview of networking, starting with OSI-7 layers. While abstracted from many developers nowadays, it is still relevant and important to know if you want a holistic view of how things work altogether.</p>"},{"location":"devops/networking/OSI%207%20Layers/#osi","title":"OSI","text":"Layers Definition Example 7.Application protocol between applications, API, software HTTP, HTTPS, FTP, DNS, SSH, RTP 6.Presentation Defines how two computers send messages/files Encryption, Character Code Translation, Compression 5.Session Defines how to open, close, manage a session Authentication, Authorization 4.Transfer Defines data transfer between two computers TCP, UDP, port number 3.Network Defines how packets are forwarded through routing IPv4, IPv6, IPSec routing 2.Datalink Defines connection betwen two physically connected nodes on a network LLC, MAC, Network switch 1.Physical Physical Cable or wireless connection transmitting binary data Cables, RJ45 <p>Suppose Commany X want to create a social media app for communication:</p> <ol> <li>The Company set up server with some computers connecting each other with cables</li> <li>Each server computer can be physically identified with an unique MAC address</li> <li>The Company admin assign unique private IP address to different servers. The ISP also assigns a public address to interact with internet</li> <li>TCP is used for machines to talk with different IP address reliably, ordered with err detect</li> <li>Session is used for authentication and authorization, ensuring communication to right person</li> <li>Data is encrypted</li> <li>Customers use their mobile app to talk with your servers, likely through HTTP</li> </ol> <p>Pizza Delivery analogy:</p> <ol> <li>The company set up physical shops and stoves for pizza ready to be made</li> <li>Each stove can be uniquely identified by manufactuer, and divided by different usage</li> <li>Different Stove groups will be located to an assigned place in the kitchen. Your restaurant will also have an address. </li> <li>Need a way to accurately tell the stove to cook the right food with no missing order</li> <li>Need a way for talking to the right person:  between Chefs, Employees, Waiters</li> <li>Need a way to make sure your talking channel is not eavesdropped, and is efficient</li> <li>Customers (and workers) can talk with each other using delivery app, website or call.</li> </ol> <p>Acronym: All People Seems to Need Data Processing</p> <p></p>"},{"location":"devops/platform%20engineering/Platform%20Overview/","title":"Platform Overview","text":""},{"location":"devops/platform%20engineering/Platform%20Overview/#intro","title":"Intro","text":"<p>This page provides a basic overview of how various technologies get pieced together in a dev platform</p> <p>Have you wondered the big picture of different tools piecing together to ship out a software in a robust platform? Why do we need such a broad range of tools when software development, in its simpliest form, is just code being written, compiled and run? This page aims to give you a holistic view in a dedicated environment setup. </p>"},{"location":"devops/platform%20engineering/Platform%20Overview/#version-control-system","title":"Version Control System","text":"<p>Problem:  How to maintain a single source of truth while collaborating with others?</p> <p>Also known as VCS, it is the system for managing changes to code or documentations. VCS is also part of the software configuration management (SCM). See wiki </p> Name Domain Interactions Characteristics Git VCS CLI Free and Open Source distributed VCS with history based on Directed Acyclic Graph (DAG) SVN VCS CLI Centralized VCS Mercurial VCS CLI Centralized VCS"},{"location":"devops/platform%20engineering/Platform%20Overview/#hosted-vcs-service","title":"Hosted VCS Service","text":"<p>Problem: Where to host that main git repository?</p> <p>With git being the most popular VCS, you still need an easily-accessible-to-all, highly available repository somewhere. Hence the hosting services, which usually couple with other features as well.</p> Name Domain Interactions Characteristics GitHub Web,CLI,API GitLab Web,CLI,API Bitbucket Also support Mercurial"},{"location":"devops/platform%20engineering/Platform%20Overview/#cicd","title":"CI/CD","text":"<p>Problem: Once the code base changes, can we make sure it can be built?</p> <p>Continuous integration (CI) is the practice of automating the integration of code changes in a centralized repository.  An Agent/runner, a computing unit somewhere, is triggered to perform the build (&amp; local test) task.</p> <p>Continuous Delivery (CD) goes one step further, deploying your changes to your infrastructure, like test environment, or promoted to staging/production</p> <p>Continuous Deployment goes all the way from code change to deploy, test, and release into production environment. Less common.</p> Type Build Basic-Test Internal-Publish Deploy-Test Promote-Staging Publish Promote-Prod Continuous integration Auto Auto Auto Continuous Delivery Auto Auto Auto Auto/Separate Auto/Separate Manual/Separate Manual/Separate Continuous Deployment Auto Auto Auto Auto Auto Auto Auto <p>Common Requirements including: - pipeline-as-code: define a series of processes as code - configuration-as-code: define setup of CI itself as code. Usually in complex environment - shared-lib: for reusing logics. Usually in enterprise for standardized method - self-host: agents/runner in your local infrastructure. Flexible and cheaper - containerized-runner: runner in a container. For better consistency - rbac: user access control is typically appreciated in enterprise environment</p> Name Domain Pipeline as code Configuration as Code Remarks GitHub Action CI YAML Circle CI CI YAML Travis CI YAML Team City CI &amp; CD YAML Kotlin DSL Jenkins CI &amp; CD Jenkinsfile JCASC plugin See Jenkins and Modern Practices Gitlab CI/CD CI &amp; CD YAML <p>For CD in particular, common requirements including:</p> <ul> <li>GitOps: Deployment is driven by Git Changes for Single source of truth, version control, compilance and more</li> <li>Kubernetes-friendly: certain CDs are marketed to be K8S friendly, for example utilizing the controller concept to achieve syncing of k8s resource with the manifest configuration</li> <li>Cloud-native: certain CDs are designed with cloud native in mind</li> <li>Other common ones, like self-host, containerized as mentioned above or open-sourced</li> </ul> Name Domain Designed for K8S? Remarks ArgoCD CD Yes FluxCD CD Yes Concourse CI &amp; CD Yes Tekton CI &amp; CD Yes Donated by Google JenkinsX CI &amp; CD Yes Spinnaker CI &amp; CD No Donated by Netflix"},{"location":"devops/platform%20engineering/Platform%20Overview/#artifactory","title":"Artifactory","text":"<p>*Problem:  Where to centralize and hosts all the required files? </p> <p>Artifacts are the concrete unit that will be used in your deployment, including:</p> <ul> <li>built code, in form of binary, jars, containers or more</li> <li>dependencies, in form of binary, jars, packages or more</li> <li>unit for distribution and deploying, such as helm charts</li> </ul> <p>Common requirement includes:</p> <ul> <li>OCI compilant:  a standardized format for creating/distributing portable units of applications</li> </ul> Names Remark JFrog Sonatype Nexus Google Artifact Registry"},{"location":"devops/platform%20engineering/Platform%20Overview/#configuration-management","title":"Configuration Management","text":"<p>*Problem: How to ensure machines are configured consistently to run programs? </p> <p>a configuration managment tool configures your (virtual) machine. Following some sort of playbook or rules, it does a one-off,  ideally idempotent change to ensure your target is configured properly</p> <p>In a push setup, changes are pushed with specific commands. In a pull setup, changes are pulled with from a centralized master agent</p> Name Code Pull/Push Remarks Ansible YAML, Python Both Puppet Ruby, DSL Pull Require master Chef Ruby, DSL Pull Require master"},{"location":"devops/platform%20engineering/Platform%20Overview/#infrastructure-as-code","title":"Infrastructure as Code","text":"<p>Problem: How to ensure infrastructure (e.g. machine) are created accordingly?</p> <p>Contrast to configuration management (CM), Infrastructe as Code (IaC) focus on creation (provisioning) of infrastructure, and keep track of the resource it create to be in state.</p> <p>TLDR: IaC create and track stuff it creates;  CM configure and change stuff.</p> Name Code Declarative? Remarks Terraform HCL Yes Most Popular CDKTerraform Python, Go... No Relatively New Pulumi Python, Go... No Based on Terraform Cloudformation YAML Yes AWS specific. DONT"},{"location":"devops/platform%20engineering/Platform%20Overview/#terraform-automation-collaboration-software","title":"Terraform Automation &amp; Collaboration Software","text":"<p>*Problem: How to execute your IaC properly</p> <p>While it's possible to use CI/CD to run IaC, a dedicated platform/portal for running IaC can be useful. They are generally described as TACOS.</p> <p>Common Requirements include:</p> <ul> <li>Drift detection: ability to detect if your infra follows your IaC</li> <li>Cost Report: everything around cost like report, estimation, alerts.</li> <li>Self service: allow others with less expertise to run IaC, with fancy UI and visuals</li> <li>Automation:  following GitOps, plan upon open PR, apply pre-merged or after-merged</li> <li>State management: someone to manage your (terraform) state remotely instead</li> <li>Compatibility: with multi-cloud, with K8S, with Terragrunt/Pulumi, or even beyond TF (Ansible)  </li> <li>Customer Support: most of these are paid services</li> </ul> Name Tailored towards Terraform Cloud Remote state management, Self-service Atlantis GitOps Pull Request Automation, Opensource, Free spacelift env0 Scalr Remote state management Digger Opensource Terraspace Opensource Cloudify <p>and of-course, this is not compulsory. Running Terraform in CI can be as simple as using the Github Action provided by Hashicorp.</p>"},{"location":"devops/platform%20engineering/Platform%20Overview/#virtual-machine-image","title":"Virtual Machine Image","text":"<p>*Problem: How to ensure consistent creation of  VM Image?</p> <p>For building Virtual machine image (e.g. AWS AMI), a combination of OS-disk and Data-disk as a reusable unit of VM, the following are the most common:</p> Name Remarks Packer Used alongside with Ansible to configure Vagrant"},{"location":"devops/platform%20engineering/Platform%20Overview/#containers","title":"Containers","text":"<p>*Problem: How to make sure your application runs consistently in any environment?</p> <p>Evolution of application hosting goes as follows: 1. Originally running directly in OS. may conflict with other apps (e.g. environment variables) 2. Hosting a full-fledged virtualized Guest OS per app, undeer Guest OS.  Resource-consuming 3. Hosting a minimal containized enivronment per app under Container Engine. 4. Complex usage of many apps (e.g. microservice) to be hosted in given servers</p> <p></p> Name Remarks Docker Most Widely Used Podman"},{"location":"devops/platform%20engineering/Platform%20Overview/#containers-nodes-orchestration","title":"Containers &amp; Nodes Orchestration","text":"<p>*Problem:  How to handle resource allocation, network, storage, operations (e.g. backup, upgrade) of different kinds of containerized apps against multiple computer nodes?</p> <p>As your service grows, scalability also becomes a complex problem. </p> Name Self-Hosted/Managed? Opensourced? Remarks Kubernetes Both Yes Most popular, originally from Google Docker Swarm self-Hosted Yes Nomad self-hosted Yes from Hashicorp"},{"location":"devops/platform%20engineering/Platform%20Overview/#secrets","title":"Secrets","text":"<p>*Problem:  How to store and centralize secret, Certs and credentials properly?</p> Name Remarks Vault AWS Secret Manager Google Secret Manager"},{"location":"devops/platform%20engineering/Platform%20Overview/#monitoring-system","title":"Monitoring System","text":"<p>*Problem:  How to monitor/troubleshoot/evaluate your platform and its deployments?</p> <p>There are multiple things within a monitoring system, but in a nutshell, it handles:</p> <ul> <li>logs - application message to let you know what's happening. Ideally structured (e.g. json)</li> <li>metrics - quantifitable data, usually aggregated, within a given period of time</li> <li>alerts - when the metrics go beyond the threshold, system need to fire warnings</li> <li>tracing - less popular but handles the tracing of a request/response</li> </ul> <p>The picture below describes the design of a basic yet comprehensive monitoring system, excluding tracing:</p> <p></p> <p>There are a lot of options to choose from, and many different kinds of combination, and you are not limited to one option either:</p> Name Purpose Remarks fluentd Log Collector fluentbit Log Collector fluentd but more lightweight and limited promtail Log Collector logstash Log Collector Frequently Used with Elasticsearch, Kibana to form ELK logging stack Prometheus Metrics Collector, Visualizations Pull based, periodically collect metrics actively Alertmanager Alerting Grafana Alerting, Visualizations, Querying Loki Log Indexing Frequently Used with promtail, Grafana as alternative of ELK Elasticsearch Log Indexing, Log Storage Kibana Visualizations, Querying buckets (s3) Log Storage Datadog everything Full package, expensive, used to be metrics-focused Splunk everything Full package, expensive, used to be log-focused <p>Tracing are not covered but that commonly includes OpenTelemetry, Jaeger, zipkin, Dynatrace, Sentry (and the 'everything' above), many of which are required to be implemented at app-level.</p>"},{"location":"devops/platform%20engineering/Platform%20Overview/#summary","title":"Summary","text":"<p>It's impossible to cover so many different technologies used for deployment in a single page, and for different use cases.  To summarize everything in a TLDR fashion, the following sample diagram demonstrate how to setup a environment from code to production</p> <p></p>"},{"location":"devops/security/Security%20Overview/","title":"Security Overview","text":"<p>This page briefly goes through the common security practice (for authentication &amp; authorization) within the industry &amp; enterprise</p>"},{"location":"devops/security/Security%20Overview/#authentication","title":"Authentication","text":"<p>Authentication is about proving your identity (e.g. user login)</p>"},{"location":"devops/security/Security%20Overview/#kerberos","title":"Kerberos","text":"<p>TODO</p> <p>**TLDR \ud83d\ude34 : exchange data between trusted hosts across untrusted network (Internet)</p>"},{"location":"devops/security/Security%20Overview/#ldap","title":"LDAP","text":"<p>Lightweight Directory Access  Protocol (LDAP) is a vendor-neutral authentication protocol that lets you store data in the LDAP directory and then authenticate users to access the directory without awareness of actual address (like a lookbook).  </p> <p>TLDR \ud83d\ude34 :  organize directories and let individuals search for the right directory</p> <p>LDAP has the following layers: * root * countries * organizations * organizational units * inidividual (including human, machines, files)</p>"},{"location":"devops/security/Security%20Overview/#sso","title":"SSO","text":"<p>single-sign on, an authentication method that enable secure access to multiple applications and services (slack, teams)  using just a set of credentials.</p> <p>TLDR \ud83d\ude34:  Make lifer easier for entering a single login page to access the rest of apps</p>"},{"location":"devops/security/Security%20Overview/#saml","title":"SAML","text":"<p>Security Assertion Markup Language (SAML) is a XML-based open source file format protocol, to tell external applications and service that a user is who they are, making SAML possible</p> <ul> <li>*identity provider: the provider that lets you does one time login to everything</li> <li>*service provider: the provider of the service (e.g. slack, teams)<ul> <li>Okta</li> <li>Microsoft</li> <li>Google</li> <li>AWS</li> <li>Saleforce</li> </ul> </li> </ul> <p>For business-to-business (B2B) focus application, SAML is very important to be implemented.</p>"},{"location":"devops/security/Security%20Overview/#oidc","title":"OIDC","text":"<p>OpenID Connect (OIDC) is an open authentication protocol that runs on top of OAuth2 authorization standard. It's purely an authentication method itself and is easier to integrate. One of its distinctive feature compared to SAML is the usage of JSON and HTTP calls over XML</p> <ul> <li>*Relying Party: Client application</li> <li>*OpenID Provider/OIDC provider/Identity Provider:  Google/Facebook that performs user authentication, user consent and <ul> <li>Okta</li> <li>Microsoft</li> <li>Google</li> <li>AWS</li> <li>Saleforce</li> </ul> </li> </ul>"},{"location":"devops/security/Security%20Overview/#authorization","title":"Authorization","text":"<p>Authorization is about confirming your (proven) identity with the right degree of access</p>"},{"location":"devops/security/Security%20Overview/#oauth2","title":"OAuth2","text":"<p>Oauth2 is an authorization framework (less rgid than a protocol) that enables applications to obtain limit access to user accounts on an HTTP services, via third party application trying to delegate access to a person's account of  gmail/facebook. </p> <p>TLDR \ud83d\ude34:  the thing behind that lets you login with Fb/Gmail, ask you access, and create account</p> <p>Common terms here:</p> <ul> <li>Authorization Server:  Google, Facebook etc</li> <li>Resource Server:  Google, Facebook server holding and sharing your personal info/photos</li> <li>Client application/Relying Party:  a third-party web/app that asks you to sign in with google/Facebook</li> <li>Resource Owner/user:  you who owns your personal info</li> </ul> <p>The Overall flow is as follows:</p> <ol> <li>User click the sign-in with google button</li> <li>The client application sends a request to Authorization Server (Google) detailing auth scope</li> <li>The Authorization Server (Google) send an approval request in redirected page</li> <li>The User click yes</li> <li>The Authorization Server (Google) send a access token to the client application</li> <li>The client application use the token to access the resource server</li> <li>Resource server validates and recognize the token, letting the client application to access without knowing user's credentials</li> </ol> <p>This makes it super popular among Business-to-Customer application (B2C), and is what causes confusion of OAuth2 being authentication method</p>"},{"location":"devops/security/Security%20Overview/#saml-auth","title":"SAML (Auth)","text":"<p>it's me SAML again!  While SAML is primarily used for authentication, the attributes within SAML assertion can include the user's roles and attributes and make authorization decisions </p>"},{"location":"devops/terraform/","title":"terraform","text":""},{"location":"devops/terraform/#terraform","title":"terraform","text":""},{"location":"devops/terraform/#sections-of-terraform","title":"Sections of terraform","text":"<ul> <li>Terraform test</li> <li>Terraform Kubernetes Module</li> </ul>"},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/","title":"Terraform Kubernetes Module","text":""},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/#intro","title":"Intro","text":"<p>While Terraform is very effective in deploying cloud infrastructure like networks and Kubernetes cluster, there's always the option of further extending the usage to apply K8S resources and charts via the Kubernetes Provider: https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs</p> <p>This page discusses the common question: Should you apply Kubernetes Resources on Terraform or not. The answer? It depends</p>"},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/#drawbacks","title":"Drawbacks","text":"<p>Let's start with the cons first, as I believe the answer for this question is normally discouraged unless there's a good reason to do it. </p>"},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/#alternatives-out-there","title":"Alternatives Out There","text":"<p>When using Terraform on top of Kubernetes, you are actually trying to achieve 2 different things:  As an additional abstraction to get the YAMLs out, and as a mechanism to do deployment.</p> <p>For the first point, Terraform is actually quite nice as a DSL with built-in functions to implement dynamic logics to a certain degree, and is more readable than <code>{{- include \"helm\" | nindent 4 }}</code> imo.  However, that also means you are giving up on tools designed for the purposes of getting Kubernetes YAML out, such as:</p> <ul> <li>Kustomize</li> <li>Helm</li> <li>Cuelang</li> <li>CDK8S</li> </ul> <p>Each with its own strength (e.g. No dependency required for Kustomize, More powerful with CDK8S), not to mention the better integration with K8S ecosystem in general.</p> <p>And that leads out to the second point: As a mechanism to do deployment. With so many Kubernetes-native CD GitOps tooling available out there such as ArgoCD and Flux, there's little reason to setup Terraform with pipelines/TACO to deploy the manifests. </p> <p>Sure, ArgoCD and Fluxcd both provide Terraform controller to apply modules, but then having Terraform in the middle of your CD and YAML just becomes unnecessary most of the time</p>"},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/#redundancy-of-functionality","title":"Redundancy of Functionality","text":"<p>When wrapping K8S manifests with Terraform, you are basically replacing the advantage of K8S manifests with the ones provided by Terraform Instead.  For example:</p> <ul> <li>K8S manifests/charts are declarative by nature, and in itself very suitable for GitOps workflow.  Using Terraform doesn't make it more declarative</li> <li>K8S already have self-recovery and reconcilation to eventually reach its desired state. You don't need Terraform to keep track of the state and resource drift</li> <li>K8S/helm charts (in combined with CDs) already support a good degree of Operations like compare differences, upgrades, rollback etc.</li> </ul>"},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/#terraform-limitation","title":"Terraform Limitation","text":"<p>Finally, there're also drawbacks of Terraform of its own, for example:</p> <ul> <li>As of 09 January 2024, Terraform provider still doesn't support proper CRs unless CRDs is deployed first  https://github.com/hashicorp/terraform-provider-kubernetes/issues/1367 </li> <li>Terraform is pretty bad at refreshing many resources under 1 module. With each resource usually equal to 1 helm chart or 1 resource kind,  and MANY namespaces available, you can easily have hundreds or thousands of Terraform resource under the same module, and take MINUTES to refresh the state.</li> <li>While you are refreshing the state of Terraform module, you cannot do another plan or apply due to state lock. A bottleneck on Terraform side rather than K8S which is designed to be scalable </li> </ul>"},{"location":"devops/terraform/Terraform%20Kubernetes%20Module/#when-to-use-it","title":"When to use it?","text":"<p>There are situations when it's appropriate to use Terraform module against K8S. For example:</p> <ul> <li>Deploying the CD itself. To solve this chicken and egg problem, Terraform make sense here</li> <li>Deploying resources with heavy references to cloud infrastructure such as <code>aws-auth</code> configmap </li> <li>Deploying a mixture of cloud infra and K8S manifests ondemand. For example, If you need to deploy a GCP service account and K8S service account utilizing workload identity per namespace, it make sense to wrap them in a Terraform module and deployed under  <code>kind: Terraform</code> CR</li> </ul> <p>and even in these cases above, Terraform is not necessarily the only way to accomplish all these tasks. </p>"},{"location":"devops/terraform/Terraform%20test/","title":"Terraform test","text":""},{"location":"devops/terraform/Terraform%20test/#introduction","title":"Introduction","text":"<p>Terraform Tests is introduced in 1.6 as a new way to validate that module configuration update do not produce breaking changes.  Before that, the defacto way of testing Terraform is Terratest , which lets you write end-to-end test through  3 stages: Deploy, Test and Teardown</p> <p>The benefits of using Terraform-test over terratest includes:</p> <ul> <li>no dependency (e.g. Go,Terratest library) required</li> <li>little to no setup required</li> <li>writes in HCL, tests in HCL</li> <li>learning and adoption is much easier</li> <li>super-easy to setup CI (literally just <code>checkout terraform</code> and <code>terraform tests</code>)</li> <li>direct access to named values (local!)</li> </ul> <p>However, the drawbacks are significant, which will be explained later. </p>"},{"location":"devops/terraform/Terraform%20test/#example","title":"Example","text":"<p>Given the following Terraform file <code>main.tf</code>:</p> <pre><code>terraform {\n  required_version = \"~&gt;1.6\"\n  backend \"local\" {\n      path = \"./terraform.tfstate\"\n  }\n  required_providers {\n    random = {\n        version = \"~&gt;3.0\"\n    }\n  }\n}\n\nvariable \"num\" {\n    description = \"A number to test fizz buzz. If null, use a random number instead\"\n    type        = number\n    default     = null\n}\n\nresource \"random_integer\" \"this\" {\n    min = 1\n    max = 100\n}\n\nlocals {\n    num    = var.num != null ? var.num : random_integer.this.result\n    fizz   = local.num % 3 == 0 ? \"fizz\" : \"\"\n    buzz   = local.num % 5 == 0 ? \"buzz\" : \"\"\n    result = trimspace(\"${local.fizz}${local.buzz}\")\n}\n\noutput \"result\" {\n    value = result\n}\n</code></pre> <p>You can run <code>terraform plan -var=num=&lt;anything&gt;</code> to verify your plan output. However, the even better way is to write a set of tests!</p> <p>Create a file named <code>main.tftest.hcl</code>. A file is pretty much considered as a test suite:</p> <pre><code># global variable allows any custom and complusory value for all `runs` within the file\n# \n# variables { key =value }\n\n# a run block can be considered as a test-case. Given it a good name!\nrun \"validate_var_num_buzz\" {\n    # set plan!\n    command = plan\n\n    # inline variable value for specific test-case\n    variables {\n      num = 95\n    }\n\n    # assert, you can have multiple blocks here\n    assert {\n      condition = output.result == \"buzz\"\n      error_message = \"95 can be divided by 5 but not 3 so should print buzz!\"\n    }\n}\n</code></pre> <p>and run <code>terraform test</code> . There, you are done! </p>"},{"location":"devops/terraform/Terraform%20test/#what-its-good-at","title":"What it's good at","text":""},{"location":"devops/terraform/Terraform%20test/#testing-complex-local-logic","title":"Testing Complex Local logic","text":"<p>One of the best thing about Terraform test, is that it lets you access named values directly, including: * Local Values * Resources Attributes</p> <p>So you don't even need to create a dedicated output, or trying to find a way to understand the standard output of your terraform plan!</p> <pre><code># This one requires apply\n\nrun \"validate_var_num_or_random_integer_logic_apply_edition\" {\n    command = apply \n    assert {\n        condition = local.num == random_integer.this.result \n        error_message = \"applied and they are not the same!\"\n    }\n}\n</code></pre> <p>so imagine you have a local block with complex logic,  like this Resourceless Terraform module that attempts to create deepmerge function in Terraform, you can easily validate them!</p>"},{"location":"devops/terraform/Terraform%20test/#testing-terraform-template-language","title":"Testing Terraform Template Language","text":"<p>Reading terraform template language is incredibly frustrating,</p> <pre><code>%{if hello_df == 1}\n    %{for hi in hello}\n        tags: ${jsondecode(concat(tags,[]))}\n    %{endfor}\n%{endif}\n</code></pre> <p>basically the same as helm-charts, and it becomes pretty much unreadable when it gets super nested while trying to expose multiple options.</p> <p>This is when Terraform test comes in rescue! You can valid them as such:</p> <pre><code>assert {\n    condition = jsondecode(kubernetes_manifest.test.yaml_body)[\"metadata\"][\"name\"] == \"one\"\n    error_message = \"not named one!\"\n}\n</code></pre>"},{"location":"devops/terraform/Terraform%20test/#setting-up-requirement","title":"Setting up Requirement","text":"<p>One of the major purposes to write a unit test is to ensure that the code will still fulfill the current requirements and does not regress in the future.  This often happens when you either have a complex type variable or a highly abstracted variable that does multiple related things at a time</p> <p>For example, if you have a  <code>user: []</code> variable that when appending a new element, does:</p> <ul> <li>create a user</li> <li>assign IAM &amp; groups to the user</li> <li>update aws-auth configmap</li> <li>creates a dedicated kubernetes namespace</li> </ul> <p>at once, you will want to write some Terraform test to ensure it's working as intended!</p>"},{"location":"devops/terraform/Terraform%20test/#things-to-watch-out-for","title":"Things to watch out for","text":"<p>Before you attempt to introduce Terraform Test in the infrastructure, consider the following limitations and pitfalls:</p>"},{"location":"devops/terraform/Terraform%20test/#destruction-against-local-terraform-apply-workflow","title":"Destruction against local Terraform Apply Workflow!","text":"<p>If following stict GitOps workflow, Automation should be the only one with permission to apply Terraform through TACOS / CICD  after a PR change.  </p> <p>Realistically however, plenty of people may still apply their infrastructure changes locally, or have an account that has permissions to do infrastructure changes.</p> <p>A <code>run</code> block with <code>command = plan</code> ommitted, actually runs <code>apply</code> by default, and does <code>terraform apply --auto-approve</code> and <code>terraform destroy --auto-approve</code> underneath with no printed logs unless set to verbose.  So imagine running a local terraform test, accidentially miss adding the <code>command = plan</code>  line, and start seeing your infrastructure getting unexpected modifications!</p> <p>Technically, you can do the same in any other testsuite like Terratest. However, the higher level of entry Barrier means it's less likely for such to happen</p>"},{"location":"devops/terraform/Terraform%20test/#not-like-an-ordinary-unit-test","title":"Not like an ordinary unit Test","text":"<p>Understanding that Terraform test is not like regular unit tests, as code coverage isn't really a thing here. In fact, as Terraform DSL language is declarative by nature, there're little value to write a test against a simple variable that's just a direct passthrough. </p> <p>Keep an good eye on what really should be tested, or else it's simply double the coding effort with very little gain in terms of infrastructure validation</p>"},{"location":"devops/terraform/Terraform%20test/#script-support-is-limited","title":"Script support is limited","text":"<p>As of todays, script support is very limited. If your infrastructure tests relies on imperative steps, such as confirming if you can access XYZ resources in private VPC through a bastion host using Session Manager SSM command,  then you are much better off using Terratest and a proper scripting languages like Go or Python instead. </p> <p>You can still have Terraform test as part of the test suite though. They are not necessarily mutually exclusive</p>"}]}